---
title: "Machine Learning"
author: "Sean Davis"
date: "June 29, 2015"
output: ioslides_presentation
---

# Preliminaries

## Install required libraries.

```{r echo=FALSE,results='hide'}
library(knitr)
options(width=60)
opts_chunk$set(tidy=TRUE,warning=FALSE,message=FALSE, cache=TRUE,
               fig.width=9,fig.height=6)
```

```{r eval=FALSE}
library(BiocInstaller)
biocLite(c('mlbench','adabag', 'e1071', 'randomForest', 'party', 'mboost', 'rpart.plot', 'formatR'))
```

```{r}
require(c('mlbench','adabag', 'e1071', 'randomForest', 'party', 'mboost', 'rpart.plot', 'formatR'))
```

# Overview

## What is machine learning?

Machine learning is a broad set of fields related to computers learning from "experience" (data). 

- Focusing on *predictive modeling* with a goal of *producing the most accurate estimates of some quantity or the most likely output of an event*. 
- These models are sometimes based on similar models for inference (testing against a null hypothesis, such as linear regression), but in many cases, predictive models are not well-suited for inference (think k-nearest-neighbor, for example). 

## The formula interface

```{r eval=FALSE}
outcome ~ var1 + var2 + ...
```

The variable `outcome` is predicted by `var1, var2, ...`

```{r eval=FALSE}
some_model_function(price ~ numBedrooms + numBaths + acres,
                 data = housingData)
```

Conveniences of the formula interface:

- Transformations such as `log10(acres)` can be specified inline. 
- Factors are converted into dummy variables automatically.

## The Non-formula interface

- The non-formula interface specifies the predictors as a matrix or data frame. 
- The outcome data are then passed into the model as a vector.

```{r eval=FALSE}
some_model_function(x = housePredictors, y = price)
```

Many R functions offer both a formula and a non-formula interface, but not all.

## General workflow for machine learning in R

1. Fit the model to a set of training data
    ```{r eval=FALSE}
    fit <- knn(trainingData, outcome, k = 5)
    ```
2. Assess the properties of the model using `print`, `plot`, `summary` or other methods
3. Predict outcomes for samples using the predict method:
    ```{r eval=FALSE}
    predict(fit, newSamples).
    ```

# Exercise 1
Playing with regression

## Is `mpg` a function of `wt`?

The formula interface in action:

```{r}
data(mtcars)
fit = lm(mpg ~ wt, data = mtcars)
summary(fit)
```

And make a plot.

```{r eval=FALSE}
plot(mpg ~ wt, data=mtcars)
abline(fit)
```

## Is `mpg` a function of `wt`?

```{r echo=FALSE}
plot(mpg ~ wt, data=mtcars)
abline(fit)
```

## Use `wt` to predict `mpg`

And predict the original data based on the fitted model.

```{r}
pred_mpg = predict(fit,mtcars)
summary(pred_mpg)
```

And look at the predicted values:

```{R eval=FALSE}
plot(mpg ~ wt, data=mtcars)
abline(fit)
points(y = pred_mpg, x = mtcars$wt, col='red')
```

## Use `wt` to predict `mpg`

```{R echo=FALSE}
plot(mpg ~ wt, data=mtcars)
abline(fit)
points(y = pred_mpg, x = mtcars$wt, col='red')
```

## Quantifying "goodness-of-fit"

```{R echo=FALSE}
plot(mpg ~ wt, data=mtcars)
abline(fit)
points(y = pred_mpg, x = mtcars$wt, col='red')
```

## Quantifying "goodness-of-fit"

- Residual Sum of Squares
    $$ RSS = \sum_{N} (y_i - f(x_i))^{2} $$

## Quantifying "goodness-of-fit"

```{r}
rss = sum((mtcars$mpg - predict(fit,mtcars))^2)
rss
anova(fit)
```

## Quantifying "goodness-of-fit"

```{r}
anova(fit)
```

## Classification Trees

As a simple dataset to try with machine learning, we are going to predict the species of 
`iris` based on four measurements.

```{r eval=FALSE}
data(iris)
View(iris)
pairs(iris[,1:4],col=iris$Species)
```

## Iris Data

```{r echo=FALSE}
data(iris)
pairs(iris[,1:4],col=iris$Species)
```


## Another slide

We can start with a simple learner, a [classification tree](https://en.wikipedia.org/wiki/Decision_tree_learning). This learner requires:

- A known class for each observation
- A set of "features" that will serve a potential predictors

1. Start with whole dataset.
2. Choose features one-at-a-time and look for a value of each variable that ends up with the most homogeneous two groups after splitting on that variable/value.
3. For each resulting group, repeat step 2 until all remaining groups have only one class in them.
4. Optionally, "prune" the tree to keep only splits that are "statistically significant".

The `party` package includes a function, `ctree` to "learn" a tree from data.

```{r}
library(party)
x = ctree(Species ~ .,data=iris)
plot(x)
```

And how well does our tree do with predicting the original classes from the data?

```{r}
library(caret)
library(e1071)
prediction = predict(x,iris)
table(prediction)
confusionMatrix(iris$Species,prediction)
```

What is the problem with what we just did to determine our prediction accurace?  

To deal with this problem, we can split the dataset into a "training" set and then check
our prediction on the other piece of the data, the "test" set.

```{r}
# choose every "odd" row for training
set.seed(42)
trainIdx = sample(c(TRUE,FALSE),size=nrow(iris),prob=c(0.2,0.8),replace=TRUE)
irisTrain = iris[trainIdx,]
# choose every "even" row for testing
irisTest  = iris[!trainIdx,]
```

Now, we can "train" our tree on the "training" set.

```{r}
trainTree = ctree(Species ~ ., data = irisTrain)
plot(trainTree)
```

And how does our `trainTree` do at predicting the original classes in the "training" data?

```{r}
library(caret)
trainPred = predict(trainTree,irisTrain)
confusionMatrix(irisTrain$Species,trainPred)
```

How is our prediction performance now on the "test" data?

```{r}
testPred = predict(trainTree,irisTest)
confusionMatrix(irisTest$Species,testPred)
```

Now, let's make this harder. We will now look at a dataset that is designed to "foil" 
tree classifiers.

```{r}
library(mlbench)
spiral = mlbench.spirals(1000,sd=0.1)
spiral = data.frame(x=spiral$x[,1],y=spiral$x[,2],class=factor(spiral$classes))
library(ggplot2)
ggplot(spiral,aes(x,y,color=class)) + geom_point()
```



```{r}
library(formatR)
trainIdx = sample(c(TRUE,FALSE),nrow(spiral),replace=TRUE,prob=c(0.5,0.5))
spiralTrain = spiral[trainIdx,]
trainTree   = ctree(class ~ .,spiralTrain)
plot(trainTree)
prediction = predict(trainTree,spiralTrain)
confusionMatrix(spiralTrain$class,prediction)
```

```{r}
spiralTest = spiral[!trainIdx,]
prediction = predict(trainTree,spiralTest)
confusionMatrix(spiralTest$class,prediction)
```

Many trees have similar prediction capability, but each is really bad.  This is a 
characteristic of a "weak learner".  Here, we see that in action by performing a bootstrap
sampling (resample with replacement), train, plot, and check prediction accuracy.

```{r}
plotBootSample = function(spiral) {
  trainIdx = sample(1:nrow(spiral),replace=TRUE)
  spiralTrain = spiral[trainIdx,]
  trainTree   = ctree(class ~ .,spiralTrain,ctree_control(minsplit=2,maxsplit=2))
  plot(trainTree)
  prediction = predict(trainTree,spiral[!trainIdx,])
  print(confusionMatrix(spiralTrain$class,prediction)$overall['Accuracy'])
}
```

```{r eval=FALSE}
# press 'ESC' to stop
while(TRUE) {
  par(ask=TRUE)
  plotBootSample(spiral)
}
```

## Boosting

We can "combine" a bunch of "weak learners", giving more "weight" to hard-to-classify observations as we build each new classifier.  In this case, we will be using the same classification tree again.

```{r}
library(adabag)
trainIdx      = sample(c(TRUE,FALSE),nrow(spiral),replace=TRUE,prob=c(0.5,0.5))
spiralTrain   = spiral[trainIdx,]
boostTree     = boosting(class ~ x + y,data = spiralTrain,control = rpart.control(maxdepth=2))
prediction    = predict(boostTree,spiralTrain)
confusionMatrix(spiralTrain$class,prediction$class)
```

```{r fig.width=9,fig.height=9}
library(rpart.plot)
par(mfrow=c(3,3),ask=FALSE)
for(i in 1:9) {
  rpart.plot(boostTree$trees[[i]])
}
```

And how does our boosted tree work on the test data?

```{r}
spiralTest = spiral[!trainIdx,]
prediction = predict(boostTree,spiralTest)
confusionMatrix(spiralTest$class,prediction$class)
```

## Random Forests

```{r}
library(randomForest)
res = randomForest(Species ~ .,data=iris)
res
```

